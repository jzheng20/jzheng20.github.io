---
permalink: /
title: " "
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---
*<strong><font size=3> â€œTheory without practice is empty, but equally, practice without theory is blind."  ---- I. Kant </font></strong>*

Hello! My name is Jingjing Zheng. My current research interests include efficient training/inference of large models grounded in theory, low-rank/sparse representation
learning with applications to efficient optimization and compute,  safety & reliability of LLMs under
resource constraints. My academic background spans art and design (B.A.), mathematics (M.S. and current Ph.D.), and computer science (completed Ph.D. degree). Since 2023, I have been pursuing my doctoral studies in Mathematics at the [University of British Columbia](https://www.ubc.ca/), under the supervision of Prof. [Yankai Cao](https://chbe.ubc.ca/yankai-cao/).



## Recent News 

*<font color=red> News! </font>* Joined the Organizing Committee of Women and Gender-diverse Mathematicians at UBC (WGM), 2025.

*<font color=red> News! </font>* Appointed to the UBC Green College Academic Committee, 2025â€“2026.

*<font color=red> News! </font>* *GradientX* has been seleted for Lab2market Validate Program, 2025 (Funded).

*<font color=red> News! </font>* Two papers have been accepted to NeurIPS 2025 (see you in San Diego!).



## Latest Projects

{% raw %}
<div style="display:flex;flex-direction:row;align-items:flex-start;margin-bottom:24px;">

  <div style="min-width:160px;max-width:160px;margin-right:20px;">
    <img src="https://raw.githubusercontent.com/jzheng20/jzheng20.github.io/master/files/AdaMSS.png"
         style="width:160px;border-radius:6px;">
  </div>

  <div style="flex:1;">

    <div style="font-size:18px;font-weight:600;margin-bottom:6px;">
      AdaMSS: Adaptive Multi-Subspace Strategy for Efficient Fine-Tuning
    </div>

    <div style="font-size:14px;">
      Ruixin Song<sup>*</sup>, Coauthor A<sup>*</sup>, Coauthor B<sup>â€ </sup><br>
      (* equal contribution, â€  equal advising)<br>
      <em>NeurIPS 2025</em><br>
      <a href="#">arxiv</a> / <a href="#">code</a> / <a href="#">bib</a>
    </div>

    <div style="font-size:14px;margin-top:10px;line-height:1.45;">
      This work introduces an adaptive sparse-computation module that learns 
      where to attend and what to extract from deep representations. It reduces 
      FLOPs by Ã—34 while preserving accuracy, enabling real-time inference on 
      large-scale benchmarks.
    </div>

  </div>

</div>
{% endraw %}






 
---

<small><em>Proud supporter of LGBTQ+ inclusion and diversity in academia. ðŸŒˆ</em></small>

 





 


 
